# Text Language Model Training Configuration

# Architecture
arch:
  name: "text_language_model@TextHRM_v1"
  loss:
    name: "text_language_model@TextLanguageModelHead"
    vocab_size: 8000
    hidden_size: 256
  
  # HRM Model Configuration - Lighter version
  H_cycles: 2
  L_cycles: 4
  H_layers: 1
  L_layers: 2
  hidden_size: 256
  expansion: 2.0
  num_heads: 4
  pos_encodings: "learned"
  halt_max_steps: 16
  halt_exploration_prob: 0.1
  forward_dtype: "bfloat16"

# Data
data_path: "data/text-tiny"

# Hyperparameters
global_batch_size: 16
epochs: 5

lr: 5e-5  # Learning rate más bajo para reducir oscilaciones
lr_min_ratio: 0.01  # Ratio mínimo más bajo
lr_warmup_steps: 500  # Menos pasos de warmup

weight_decay: 0.001  # Weight decay más bajo para estabilidad
beta1: 0.9
beta2: 0.99  # Beta2 más bajo para reducir oscilaciones

# Puzzle embedding
puzzle_emb_lr: 1e-4
puzzle_emb_weight_decay: 0.01

# Names
project_name: "Text-LLM-HRM"
run_name: null  # Will be auto-generated
checkpoint_path: null  # Will be auto-generated

# Extras
seed: 42
checkpoint_every_eval: true
eval_interval: 2
eval_save_outputs: ["logits", "predictions"] 